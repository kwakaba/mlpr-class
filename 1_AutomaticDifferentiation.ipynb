{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy Functions\n",
    "\n",
    "The following sample code demonstrates the automatic differentiation of $y = (\\log 2x)^2$ regarding $x$.\n",
    "\n",
    "In PyTorch, every data is represented as `torch.Tensor` object.\n",
    "The basic operations can be expressed in a natural way as follows, but **the operations yield nodes of the computational graph** that remember other nodes connected to it, so the produced variables such as `a`, `b`, `y` are also `torch.Tensor` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.nn.Parameter(torch.tensor(3.0))\n",
    "a = 2 * x\n",
    "b = torch.log(a)\n",
    "y = b * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(a)\n",
    "print(b)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the gradient of $y$ regarding $x$ on the computational graph just by calling backward() function, which is equipped on Tensor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We can check the value of the gradient in the vicinity of $x = 3$ like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For another example, let's compute the derivation of $y = x^2 \\log x$ regarding $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.nn.Parameter(torch.tensor(3.0))\n",
    "a = x * x\n",
    "b = torch.log(x)\n",
    "y = a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(a)\n",
    "print(b)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the result is the same as the value shown in the lecture slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Consider a regression task that has two-dimensional feature vectors as input.\n",
    "The number of training instances is $N = 30$.\n",
    "The true formula that describes the relationship between $x$ and $y$ is $y = 2x_1 - 1.5x_2 + 5$.\n",
    "First, we make a toy dataset on this condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.randn(N, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_true = xs @ torch.Tensor([2.0, -1.5]) + 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used **matrix multiplication operator** `@`, which is a new Python feature from Python 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_true = ys_true.view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For compatibility with the specification of loss functions we will see later, the `ys_true` should be a $30 \\times 1$ matrix, rather than a simple $30$ dimensional vector.\n",
    "`view()` method reshapes the tensor.\n",
    "If you provide `-1` to `view()` method, it uses the original size for that dimension (in this case, `30`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the linear transformation model\n",
    "\n",
    "The important thing you should notice is that the instantiation of `nn.Linear` involves **the instantiation of parameters w and b as nodes in a computational graph**. They are initialized at random in a default setting.\n",
    "In this task, we need a linear function that maps two-dimensional vector to one-dimensional vector (scalar), so we create `Linear(2, 1)`.\n",
    "https://pytorch.org/docs/master/nn.html#torch.nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in linear.parameters():\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compute the prediction of $y$ using the model that has the initial parameters in this way.\n",
    "\n",
    "Note that **this code implicitly creates a node on the computational graph**.\n",
    "`ys_pred` is not just a variable that stores an actual value of y.\n",
    "It is a variable that represents a node in the computational graph, which remembers that there are edges from x, w and b to this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_pred = linear(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply a loss function\n",
    "\n",
    "PyTorch also provides a lot of loss functions.\n",
    "One of them is the Mean Square Error (`MSELoss`), which is usually used in regression tasks.\n",
    "https://pytorch.org/docs/master/nn.html#torch.nn.MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "loss = mse(ys_pred, ys_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation\n",
    "\n",
    "You can calculate **the gradient of the loss function regarding the parameters on the computational graph** just by calling `backward()` function, which is equipped on `Tensor` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!\n",
    "You can check the value of the gradient in the vicinity of the current parameters like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in linear.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization of Linear Regression\n",
    "\n",
    "We can apply the gradient descent algorithm to optimize the model parameters.\n",
    "PyTorch also provides a lot of implementations of numerical optimizer including `SGD` (stochastic gradient descent, which is a generalization of gradient descent).\n",
    "https://pytorch.org/docs/master/optim.html#torch.optim.SGD\n",
    "\n",
    "In this code, we iterate `300` times to update the parameters. In each iteration, we do:\n",
    "1. compute the prediction of $y$ as a node in the computational graph for all data by `linear(xs)`\n",
    "2. compute the loss function as a node in the computational graph by `mse(ys_pred, ys_true)` (The step 1 and 2 complete the forward computation that is required to calculate the gradient in the back propagation)\n",
    "3. reset gradient values to be zeros by calling `optimizer.zero_grad()`\n",
    "4. call `backward()`\n",
    "5. call the parameter updating method `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(2, 1)\n",
    "mse = nn.MSELoss()\n",
    "optimizer = optim.SGD(linear.parameters(), lr = 0.1)\n",
    "for epoch in range(300):\n",
    "    ys_pred = linear(xs)\n",
    "    loss = mse(ys_pred, ys_true)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we got the perfect estimation of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in linear.parameters():\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Consider a classification task that has three-dimensional feature vectors $(x_1, x_2, x_3)$ as input.\n",
    "The number of training instances is $N = 30$.\n",
    "A data instance belonging to positive class tends to have a larger value for $x_1$ but a lower value for $x_2$.\n",
    "A data instance belonging to negative class tends to have a lower value for $x_1$ but a higher value for $x_2$.\n",
    "First, we make a toy dataset on this condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Np = 15\n",
    "Nn = 15\n",
    "N = Np + Nn\n",
    "xps = torch.randn(Np, 3) + torch.Tensor([2.0, -2.0, 0.0])\n",
    "xns = torch.randn(Nn, 3) + torch.Tensor([-2.0, 2.0, 0.0])\n",
    "xs = torch.cat((xps, xns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_true = torch.cat((torch.ones(Np), torch.zeros(Nn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_true = ys_true.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model optimization\n",
    "\n",
    "In this task, we need a linear function that maps three-dimensional vector to one-dimensional vector (scalar), so we create `Linear(3, 1)` that represents a linear model $\\hat{y} = w_1x_1 + w_2x_2 + w_3x_3 + b$ which has four parameters $w_1, w_2, w_3$ and $b$.\n",
    "**These parameters are defined and stored as objects that represents nodes in a computational graph inside the linear object**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in linear.parameters():\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a convenient loss function `BCEWithLogitsLoss` that first applies the sigmoid function and then compute the \"binary cross entropy (BCE)\". Check https://pytorch.org/docs/master/nn.html#torch.nn.BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_with_sigmoid = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(linear.parameters(), lr = 0.1)\n",
    "for epoch in range(300):\n",
    "    zs = linear(xs)\n",
    "    loss = bce_with_sigmoid(zs, ys_true)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in linear.parameters():\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the prediction of the classification for any data by using the trained logistic regression model as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_pred = F.sigmoid(linear(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
