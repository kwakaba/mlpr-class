{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2LbyXmJTZVm",
    "outputId": "a7b6c00b-624b-480b-8bd3-ecfaa79221df"
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install portalocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKWD-JhCTZVu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtFGIlYVTZVv"
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torchtext.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fEw_qcYTZVw"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SST2 is a dataset with positive/negative sentiment labels.\n",
    "# （SST2は、正負の感情ラベルを持つデータセットである。）\n",
    "# Label numbers mean: 0=negative, 1=positive\n",
    "\n",
    "from torchtext.datasets import SST2\n",
    "train = SST2(split='train').map(lambda x: (x[0], str(x[1])))\n",
    "test = SST2(split='dev').map(lambda x: (x[0], str(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code to use IMDb, which is a movie review dataset with positive/negative sentiment tags.\n",
    "# While SST2 is a stream of (text, label) pairs, IMDb is of (label, text) pairs, so we apply map to align the type of the stream.\n",
    "# See the following site to check the structure of other datasets:\n",
    "# （ポジティブ/ネガティブの感情タグ付きの映画のレビューデータセットであるIMDbを用いる場合は以下のコードを用いる．\n",
    "# 　SST2は(text, label)のペアのストリームですが、IMDbは(label, text)のペアなので、ストリームの型を揃えるために、mapを適用します。\n",
    "# 　他のデータセットの構造を確認したい場合は、以下のサイトを参照してください。）\n",
    "# https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "# from torchtext.datasets import IMDB\n",
    "# train = IMDB(split='train').map(lambda x: (x[1], str(x[0])))\n",
    "# test = IMDB(split='test').map(lambda x: (x[1], str(x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code to use AG News, which is a news article dataset with genre tags.\n",
    "# （以下のコードを実行すると、ジャンルタグ付きのニュース記事データセットであるAG Newsが利用できます。）\n",
    "# Labels numbers mean: 1=“World”, 2=“Sports”, 3=“Business”, 4=“Sci/Tech”\n",
    "\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train = AG_NEWS(split='train').map(lambda x: (x[1], str(x[0])))\n",
    "test = AG_NEWS(split='test').map(lambda x: (x[1], str(x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code to use your own dataset.\n",
    "# This code assumes that you have train.tsv and test.tsv files in the data/my_text_dataset folder, \n",
    "# which contain tab-delimited pairs of \"text\" and \"label\" for each line.\n",
    "# （自作のデータセットを用いる場合は，以下のコードを用いる．\n",
    "# 　このコードでは，data/my_text_dataset フォルダに，タブ区切りで「テキスト」と「ラベル」を\n",
    "# 　1行に1組ずつ列挙した train.tsv および test.tsv のファイルがあることを想定している．）\n",
    "\n",
    "# import pandas as pd\n",
    "# train = pd.read_table('./data/my_text_dataset/train.tsv', header=None).values\n",
    "# test = pd.read_table('./data/my_text_dataset/test.tsv', header=None).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "token_counter = Counter()\n",
    "label_counter = Counter()\n",
    "for text, label in train:\n",
    "    token_counter.update(tokenize(text))\n",
    "    label_counter[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "text_vocab = vocab(token_counter, min_freq=2, specials=['<unk>', '<pad>', '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use a pre-trained word embedding vector, run the following code.\n",
    "# （事前学習済みの単語埋め込みベクトルを用いる場合は，以下のコードを用いる．）\n",
    "\n",
    "# glove = torchtext.vocab.GloVe()\n",
    "# text_vocab = vocab(glove.stoi, specials=['<unk>', '<pad>', '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocab.set_default_index(text_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vocab = vocab(label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(text_vocab.get_stoi().items())[:20])\n",
    "print(label_vocab.get_stoi().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform = T.Sequential(\n",
    "    T.VocabTransform(text_vocab),\n",
    "    T.Truncate(50-2),\n",
    "    T.AddToken(token=text_vocab['<bos>'], begin=True),\n",
    "    T.AddToken(token=text_vocab['<eos>'], begin=False),\n",
    "    T.ToTensor(padding_value=text_vocab['<pad>'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    texts = text_transform([tokenize(text) for (text, label) in batch])\n",
    "    labels = torch.tensor([label_vocab[label] for (text, label) in batch])\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train, batch_size=4, shuffle=True, collate_fn=collate_batch)\n",
    "testloader = DataLoader(test, batch_size=4, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = trainloader.__iter__().__next__()\n",
    "x, y = data\n",
    "print(x)\n",
    "print(y)\n",
    "for x_i, y_i in zip(x, y):\n",
    "    print(text_vocab.lookup_tokens(list(x_i)))\n",
    "    print(label_vocab.lookup_token(y_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64TsbX1yTZV2"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        # If you want to use a pre-trained word embedding vector, insert the following code.\n",
    "        # （事前学習済みの単語埋め込みベクトルを用いる場合は，以下のコードを挿入する．）\n",
    "        # self.embedding = nn.Embedding.from_pretrained(glove.vectors, freeze=True)\n",
    "        self.lstm = nn.LSTM(self.embedding.embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        hn = hn.squeeze(0)\n",
    "        return self.fc(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IdpeEyvATZV2",
    "outputId": "15209086-665f-4ea2-defe-659a64edef88",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "rnn = RNN(len(text_vocab), 100, 30, len(label_vocab))\n",
    "rnn.to(device)\n",
    "optimizer = optim.SGD(rnn.parameters(), lr = 0.1)\n",
    "\n",
    "for epoch in range(50):\n",
    "    sumloss = 0.0\n",
    "    # In an environment with sufficient computing resources, it is better to use all the data.\n",
    "    # （計算資源が十分ある環境では，全てのデータを使う方が良い）\n",
    "    #for data in trainloader:  # Using all batches\n",
    "    for data in islice(trainloader, 250): # Using only 250 batches\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        a = rnn(x)\n",
    "        loss = F.cross_entropy(a, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sumloss += loss.item()\n",
    "    print('epoch: {}, loss: {:.4f}'.format(epoch, sumloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmQFOsDNTZV3",
    "outputId": "f6d8df93-a39f-400c-aaa6-940d25662af0"
   },
   "outputs": [],
   "source": [
    "testloader.sort = False\n",
    "testloader.sort_within_batch = False\n",
    "ys = []\n",
    "pred_ys = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        a = rnn(x)\n",
    "        pred_y = torch.argmax(a, dim=1)\n",
    "        ys += [y_i.item() for y_i in y]\n",
    "        pred_ys += [y_i.item() for y_i in pred_y]\n",
    "\n",
    "print((pred_y == y).sum().item() / pred_y.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(ys, pred_ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yf9AxkjhTZV3"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-90TCI21TZV4",
    "outputId": "eeb495c2-b8e6-4058-c6e8-137db41820cc"
   },
   "outputs": [],
   "source": [
    "data = dataiter.__next__()\n",
    "x = data[0].to(device)\n",
    "y = data[1].to(device)\n",
    "a = rnn(x)\n",
    "pred_y = torch.argmax(a, dim=1)\n",
    "for x_i, y_i, pred_y_i in zip(x, y, pred_y):\n",
    "    print(' '.join(text_vocab.lookup_tokens(list(x_i))))\n",
    "    print('true:' + label_vocab.lookup_token(y_i))\n",
    "    print('pred:' + label_vocab.lookup_token(pred_y_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
