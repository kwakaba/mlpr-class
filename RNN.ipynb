{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2LbyXmJTZVm",
    "outputId": "a7b6c00b-624b-480b-8bd3-ecfaa79221df"
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKWD-JhCTZVu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtFGIlYVTZVv"
   },
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fEw_qcYTZVw"
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "TEXT = torchtext.legacy.data.Field(sequential=True, batch_first=True, tokenize=tokenizer, lower=True)\n",
    "LABEL = torchtext.legacy.data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XAlSgrCTZVx"
   },
   "outputs": [],
   "source": [
    "train, valid, test = torchtext.legacy.datasets.SST.splits(TEXT, LABEL, root='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZysDVUCRTZVx"
   },
   "outputs": [],
   "source": [
    "# To use IMDb, which is a movie review dataset with positive/negative sentiment tags, the following code is used.\n",
    "# （ポジティブ/ネガティブの感情タグ付きの映画のレビューデータセットであるIMDbを用いる場合は以下のコードを用いる．）\n",
    "#\n",
    "# train, test = torchtext.legacy.datasets.IMDB.splits(TEXT, LABEL, root='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiIZkYPgTZVy"
   },
   "outputs": [],
   "source": [
    "# To use your own dataset, use the following code.\n",
    "# This code assumes that you have train.tsv and test.tsv files in the data/my_text_dataset folder, \n",
    "# which contain tab-delimited pairs of \"text\" and \"label\" for each line.\n",
    "# （自作のデータセットを用いる場合は，以下のコードを用いる．\n",
    "# このコードでは，data/my_text_dataset フォルダに，タブ区切りで「テキスト」と「ラベル」を\n",
    "# 1行に1組ずつ列挙した train.tsv および test.tsv のファイルがあることを想定している．）\n",
    "# train, test = torchtext.legacy.data.TabularDataset.splits(path='./data/my_text_dataset',\n",
    "#                                          train='train.tsv', test='test.tsv', format='tsv',\n",
    "#                                          fields=[('text', TEXT), ('label', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpojfoOfTZVz"
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, max_size=25000)\n",
    "# To use a pre-trained word embedding vector, use the following code.\n",
    "# （事前学習済みの単語埋め込みベクトルを用いる場合は，以下のコードを用いる．）\n",
    "# TEXT.build_vocab(train, vectors=\"glove.6B.100d\")\n",
    "\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQ69PWeCTZV0",
    "outputId": "c6d4ce21-5f0e-45f8-ef79-0184164a20d6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(LABEL.vocab.stoi.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kox-xZjMTZV1",
    "outputId": "1b539e97-7952-4756-9a91-ac401091477c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(list(TEXT.vocab.stoi.items())[:20])\n",
    "print(len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27JOx-AKTZV1"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "trainloader, testloader = torchtext.legacy.data.BucketIterator.splits((train, test), batch_size=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "md-yz9kpTZV1"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8iFme-EYTZV2",
    "outputId": "76a35572-3935-4e03-e4b4-9e690e17e587"
   },
   "outputs": [],
   "source": [
    "data = dataiter.__next__()\n",
    "x, y = data.text, data.label\n",
    "for x_i in x:\n",
    "    print(' '.join(TEXT.vocab.itos[w] for w in x_i))\n",
    "print([LABEL.vocab.itos[yi] for yi in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64TsbX1yTZV2"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        hn = hn.squeeze(0)\n",
    "        return self.fc(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IdpeEyvATZV2",
    "outputId": "15209086-665f-4ea2-defe-659a64edef88",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "rnn = RNN(len(TEXT.vocab), 100, 30, 3)\n",
    "# If you want to use a pre-trained word embedding vector, insert the following code.\n",
    "# （事前学習済みの単語埋め込みベクトルを用いる場合は，以下のコードを挿入する．）\n",
    "# rnn.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "rnn.to(device)\n",
    "optimizer = optim.SGD(rnn.parameters(), lr = 0.01)\n",
    "for epoch in range(10):\n",
    "    sumloss = 0.0\n",
    "    # In an environment with sufficient computing resources, it is better to use all the data.\n",
    "    # （計算資源が十分ある環境では，全てのデータを使う方が良い）\n",
    "    #for data in trainloader:  （計算資源が十分ある環境では，全てのデータを使う方が良い）\n",
    "    for data in islice(trainloader, 250): # Using only 250 batches\n",
    "        x, y = data.text, data.label - 1\n",
    "        optimizer.zero_grad()\n",
    "        a = rnn(x)\n",
    "        loss = F.cross_entropy(a, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sumloss += loss.item()\n",
    "    print('epoch: {}, loss: {:.4f}'.format(epoch, sumloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmQFOsDNTZV3",
    "outputId": "f6d8df93-a39f-400c-aaa6-940d25662af0"
   },
   "outputs": [],
   "source": [
    "testloader.sort = False\n",
    "testloader.sort_within_batch = False\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        x, y = data.text, data.label - 1\n",
    "        a = rnn(x)\n",
    "        pred_y = torch.argmax(a, dim=1)\n",
    "        correct += (pred_y == y).sum().item()\n",
    "        total += pred_y.size(0)\n",
    "\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yf9AxkjhTZV3"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-90TCI21TZV4",
    "outputId": "eeb495c2-b8e6-4058-c6e8-137db41820cc"
   },
   "outputs": [],
   "source": [
    "data = dataiter.__next__()\n",
    "x, y = data.text, data.label\n",
    "for x_i in x:\n",
    "    print(' '.join(TEXT.vocab.itos[w] for w in x_i))\n",
    "a = rnn(x)\n",
    "pred_y = torch.argmax(a, dim=1)\n",
    "print([LABEL.vocab.itos[yi + 1] for yi in pred_y])\n",
    "print([LABEL.vocab.itos[yi] for yi in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
