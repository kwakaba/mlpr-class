{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "TEXT = torchtext.data.Field(sequential=True, batch_first=True, tokenize=tokenizer, lower=True)\n",
    "LABEL = torchtext.data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = torchtext.datasets.SST.splits(TEXT, LABEL, root='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use IMDb, which is a movie review dataset with positive/negative sentiment tags, the following code is used.\n",
    "# （ポジティブ/ネガティブの感情タグ付きの映画のレビューデータセットであるIMDbを用いる場合は以下のコードを用いる．）\n",
    "#\n",
    "# train, test = torchtext.datasets.IMDB.splits(TEXT, LABEL, root='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use your own dataset, use the following code.\n",
    "# This code assumes that you have train.tsv and test.tsv files in the data/my_text_dataset folder, \n",
    "# which contain tab-delimited pairs of \"text\" and \"label\" for each line.\n",
    "# （自作のデータセットを用いる場合は，以下のコードを用いる．\n",
    "# このコードでは，data/my_text_dataset フォルダに，タブ区切りで「テキスト」と「ラベル」を\n",
    "# 1行に1組ずつ列挙した train.tsv および test.tsv のファイルがあることを想定している．）\n",
    "# \n",
    "# **Note**: In the following code, the text and labels of the batch (i.e., data) are accessed as\n",
    "# data.text and data.label, but when using the TabularDataset class, these should be replaced with\n",
    "# data.Text and data.Label. \n",
    "# As of June 24, 2020, the variable names in torchtext seem to be inconsistent.\n",
    "# If you don't rewrite it, you will get the error `'Batch' object has no attribute 'text'`.\n",
    "# （**注意**：以降のコードでは，バッチ（変数名を data とする）のテキストやラベルを data.text や data.label として\n",
    "# アクセスしているが，TabularDataset クラスを用いる場合には，これを data.Text や data.Label と書き換えること．\n",
    "# 2020年6月24日時点で，torchtextの中で変数名が一貫していないようである．\n",
    "# 書き換えないと `'Batch' object has no attribute 'text'` というエラーが出る．）\n",
    "#\n",
    "# train, test = torchtext.data.TabularDataset.splits(path='./data/my_text_dataset',\n",
    "#                                          train='train.tsv', test='test.tsv', format='tsv',\n",
    "#                                          fields=[('Text', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, max_size=25000)\n",
    "# To use a pre-trained word embedding vector, use the following code.\n",
    "# （事前学習済みの単語埋め込みベクトルを用いる場合は，以下のコードを用いる．）\n",
    "# TEXT.build_vocab(train, vectors=\"glove.6B.100d\")\n",
    "\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(LABEL.vocab.stoi.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(list(TEXT.vocab.stoi.items())[:20])\n",
    "print(len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "trainloader, testloader = torchtext.data.BucketIterator.splits((train, test), batch_size=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataiter.__next__()\n",
    "x, y = data.text, data.label\n",
    "for x_i in x:\n",
    "    print(' '.join(TEXT.vocab.itos[w] for w in x_i))\n",
    "print([LABEL.vocab.itos[yi] for yi in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        hn = hn.squeeze(0)\n",
    "        return self.fc(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "rnn = RNN(len(TEXT.vocab), 100, 30, 3)\n",
    "# If you want to use a pre-trained word embedding vector, insert the following code.\n",
    "# （事前学習済みの単語埋め込みベクトルを用いる場合は，以下のコードを挿入する．）\n",
    "# rnn.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "rnn.to(device)\n",
    "optimizer = optim.SGD(rnn.parameters(), lr = 0.01)\n",
    "for epoch in range(10):\n",
    "    sumloss = 0.0\n",
    "    # In an environment with sufficient computing resources, it is better to use all the data.\n",
    "    # （計算資源が十分ある環境では，全てのデータを使う方が良い）\n",
    "    #for data in trainloader:  （計算資源が十分ある環境では，全てのデータを使う方が良い）\n",
    "    for data in islice(trainloader, 250): # Using only 250 batches\n",
    "        x, y = data.text, data.label - 1\n",
    "        optimizer.zero_grad()\n",
    "        a = rnn(x)\n",
    "        loss = F.cross_entropy(a, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sumloss += loss.item()\n",
    "    print('epoch: {}, loss: {:.4f}'.format(epoch, sumloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        x, y = data.text, data.label - 1\n",
    "        a = rnn(x)\n",
    "        pred_y = torch.argmax(a, dim=1)\n",
    "        correct += (pred_y == y).sum().item()\n",
    "        total += pred_y.size(0)\n",
    "\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataiter.__next__()\n",
    "x, y = data.text, data.label\n",
    "for x_i in x:\n",
    "    print(' '.join(TEXT.vocab.itos[w] for w in x_i))\n",
    "a = rnn(x)\n",
    "pred_y = torch.argmax(a, dim=1)\n",
    "print([LABEL.vocab.itos[yi + 1] for yi in pred_y])\n",
    "print([LABEL.vocab.itos[yi] for yi in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
